{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama Index Demo 및 발표\n",
    "### 디지털애널리틱스융합협동과정 2023311??? 최민철 <br> 디지털애널리틱스융합협동과정 2023311561 이정환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Knowledge\n",
    "- **LLM에서 학습되지 않은 데이터 : 사내 데이터, 개인 데이터**  \n",
    "- **추가 학습 없이 : 데이터 참조 형식**\n",
    "- **다양한 데이터 형식 : pdf 등 파일형식, Youtube와 같은 웹 서비스**\n",
    "\n",
    "## RAG(Retrieval Augmented Generation) vs FineTuning\n",
    "<!-- ![rag.png](./image/RAG.png) -->\n",
    "<!-- ![finetuning.png](./image/finetuning.png) -->\n",
    "![rag_vs_ft.png](./image/rag_vs_finetuning.png)\n",
    "\n",
    "## RAG 5 단계\n",
    "![rag_five_stage.png](./image/rag_five_stage.png)\n",
    "\n",
    "- **로딩(Loading)**: 데이터를 소스에서 파이프라인으로 가져오는 단계. (LlamaHub)\n",
    "- **인덱싱(Indexing)**: 데이터 쿼리를 가능하게 하는 데이터 구조를 생성하는 과정. 주로 벡터 임베딩 생성.\n",
    "- **저장(Storing)**: 인덱스와 다른 메타데이터 저장. 매번 인덱싱할 필요 없음.\n",
    "- **쿼리(Querying)**: LLM과 LlamaIndex 데이터 구조를 사용하여 쿼리 수행.\n",
    "- **평가(Evaluation)**: 쿼리에 대한 응답의 정확성, 충실도, 속도 측정.\n",
    "\n",
    "## 종합적인 사용 사례\n",
    "- **쿼리 엔진**: 데이터에 대해 자연어 쿼리를 통해 질문하여 참조 컨텍스트를 LLM에 전달하는 엔드 투 엔드 파이프라인\n",
    "- **채팅 엔진**: 데이터와의 대화를 위한 엔드 투 엔드 파이프라인\n",
    "- **에이전트**: LLM으로 구동되는 자동화된 의사 결정자\n",
    "\n",
    "\n",
    "<br><br>\n",
    "<참고자료>  \n",
    "https://medium.com/neo4j/knowledge-graphs-llms-fine-tuning-vs-retrieval-augmented-generation-30e875d63a35  \n",
    "https://gpt-index.readthedocs.io/en/stable/  \n",
    "https://llamahub.ai/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama Index VS Langchain\n",
    "\n",
    "- **라마인덱스는 랭체인 라이브러리로 구축된 애플리케이션**\n",
    "- **랭체인은 조금 더 전반적인 기능들을 다룰 수 있음. loading, processing, indexing, LLM과의 interaction 등. 이로써 사용자는 사용자의 의도에 따라 커스터마이징에 유리함**\n",
    "- **반면, 라마인덱스는 search and retrieval에 초점을 맞춘 애플리케이션으로 거대한 양의 데이터를 처리하기에 좋음**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# requirements settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/jerryjliu/llama_index.git\n",
    "# %pip install poetry\n",
    "# %cd llama_index\n",
    "# %poetry install\n",
    "# %pip install -U llama-index\n",
    "# %pip install -U langchain\n",
    "# %pip install sentence-transformers\n",
    "# %pip install accelerate\n",
    "# %pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "# %pip install transformers==4.34.0\n",
    "# %pip install faiss-cpu\n",
    "# %pip install lamma-hub\n",
    "# %pip install youtube_transcript_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#로그 설정 & API Key 설정\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "import os \n",
    "\n",
    "# load .env\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG, force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 로드\n",
    "\n",
    "사용하고 싶은 데이터(문서) 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:llama_index.readers.file.base:> [SimpleDirectoryReader] Total files added: 7\n"
     ]
    }
   ],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM\n",
    "\n",
    "\n",
    "- **Default : OpenAI의 chatgpt text-davinci-003**\n",
    "- **For Demo : gpt-3.5-turbo**\n",
    "- **OpenAI의 다른 모델 (e.g. gpt-3.5-turbo 등), Custom Model, huggingface 모델도 사용 가능**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='/Users/sean_forhim/opt/anaconda3/envs/llama_index/lib/python3.9/site-packages/certifi/cacert.pem'\n",
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='/Users/sean_forhim/opt/anaconda3/envs/llama_index/lib/python3.9/site-packages/certifi/cacert.pem'\n"
     ]
    }
   ],
   "source": [
    "from llama_index import LLMPredictor\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0.7, model_name=\"gpt-3.5-turbo\"))\n",
    "\n",
    "\n",
    "# from langchain.llms.base import LLM\n",
    "# from typing import Optional, List, Mapping, Any\n",
    "# from transformers import pipeline\n",
    "\n",
    "# # setting\n",
    "# context_window = 2048\n",
    "# num_output = 256\n",
    "# model_name = 'beomi/KoAlpaca-llama-1-7b'\n",
    "# pipe = pipeline(\"text-generation\", model=model_name)\n",
    "\n",
    "# class CustomLLM(LLM):\n",
    "#     def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "#         prompt_length = len(prompt)\n",
    "#         response = pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
    "\n",
    "#         # only return newly generated tokens\n",
    "#         return response[prompt_length:]\n",
    "\n",
    "#     @property\n",
    "#     def _identifying_params(self) -> Mapping[str, Any]:\n",
    "#         return {\"name_of_model\": model_name}\n",
    "\n",
    "#     @property\n",
    "#     def _llm_type(self) -> str:\n",
    "#         return \"custom\"\n",
    "\n",
    "    \n",
    "# # define our LLM\n",
    "# llm_predictor = LLMPredictor(llm=CustomLLM())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding\n",
    "\n",
    "- **Default : OpenAI의 text-embedding-ada-002**\n",
    "- **For Demo : Huggingface (bongsoo/moco-sentencedistilbertV2.1)**\n",
    "- **custom model, huggingface 모델이 사용 가능**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: bongsoo/moco-sentencedistilbertV2.1\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device: cpu\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from llama_index.embeddings import LangchainEmbedding\n",
    "\n",
    "\n",
    "embed_model = LangchainEmbedding(HuggingFaceEmbeddings(model_name='bongsoo/moco-sentencedistilbertV2.1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataBase\n",
    "\n",
    "- **Default : GPTVectorStoreIndex (Python Dict)**\n",
    "- **For Demo : Faiss**\n",
    "- **Vector DataBase : Faiss, Qdrant, Chroma, Milvus (이상 패키지로 제공하는 데이터베이스), Pinecone, Weaviate (이상 클라우드 서비스로 제공하는 데이터베이스)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:faiss.loader:Loading faiss.\n",
      "INFO:faiss.loader:Successfully loaded faiss.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "import faiss\n",
    "\n",
    "\n",
    "# faiss의 indexing : IndexFlatL2, IndexFlatIP, IndexIVFFlat\n",
    "faiss_index = faiss.IndexFlatL2(768)\n",
    "vector_store = FaissVectorStore(faiss_index=faiss_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index\n",
    "- **앞선 모듈 객체화 이후 Index 생성**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): openaipublic.blob.core.windows.net:443\n",
      "DEBUG:urllib3.connectionpool:https://openaipublic.blob.core.windows.net:443 \"GET /encodings/cl100k_base.tiktoken HTTP/1.1\" 200 1681126\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: 제1장: 데이터 프론트\n",
      "\n",
      "밤이 되면 반짝이는 네오 도쿄. 고층 빌딩이 늘어서고, 네온...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: 제2장: 울프 코퍼레이션의 함정\n",
      "\n",
      "미코는 목적지인 술집 '할머니의 집'으로 향하는 길...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: 제3장: 배신과 재회\n",
      "\n",
      "술집 '할머니의 집'에서 미코는 데이터를 받을 사람인 료를 기...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: 제4장: 울프 코퍼레이션의 붕괴\n",
      "\n",
      "미코와 료는 해커 집단과 함께 울프 코퍼레이션에 대...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: 제5장: 결전의 순간\n",
      "\n",
      "미코와 료는 마침내 울프 코퍼레이션의 최상층에 도착해 CEO인...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: 제6장: 진실의 해방\n",
      "\n",
      "미코는 울프 박사의 약점을 파고들어 그를 쓰러뜨리는데 성공한다...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: 제7장: 새로운 시작\n",
      "\n",
      "울프 코퍼레이션이 무너진 후, 미코와 료는 서로의 과거를 용서...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e3158e9c684064bfdc6b7c960fa66e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index import GPTVectorStoreIndex, ServiceContext, StorageContext\n",
    "\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor,embed_model=embed_model)\n",
    "index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Engine\n",
    "- **From Index : 사용자의 쿼리와 입력된 정보**\n",
    "- **From LLM : 해당 정보와 입력을 바탕으로 응답 생성**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e28b599f0fa44f8d84bef0dd337dc2d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:llama_index.indices.utils:> Top 2 nodes:\n",
      "> [Node 5] [Similarity score:             47.2367] 제6장: 진실의 해방\n",
      "\n",
      "미코는 울프 박사의 약점을 파고들어 그를 쓰러뜨리는데 성공한다. 그리고 해커 집단과 함께 울프 코퍼레이션의 악행을 세상에 공개하고 시민들을 해방시킨다....\n",
      "> [Node 4] [Similarity score:             48.4738] 제5장: 결전의 순간\n",
      "\n",
      "미코와 료는 마침내 울프 코퍼레이션의 최상층에 도착해 CEO인 교활한 울프 박사와 대면한다. 울프 박사는 시민을 지배하려는 사악한 야망을 드러내며 자신...\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': \"You are an expert Q&A system that is trusted around the world.\\nAlways answer the query using the provided context information, and not prior knowledge.\\nSome rules to follow:\\n1. Never directly reference the given context in your answer.\\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\"}, {'role': 'user', 'content': 'Context information is below.\\n---------------------\\nfile_path: data/akazukin6.txt\\n\\n제6장: 진실의 해방\\n\\n미코는 울프 박사의 약점을 파고들어 그를 쓰러뜨리는데 성공한다. 그리고 해커 집단과 함께 울프 코퍼레이션의 악행을 세상에 공개하고 시민들을 해방시킨다. 이 승리로 미코의 어머니의 치료법도 찾아내고, 그녀의 병은 완치된다.\\n\\nfile_path: data/akazukin5.txt\\n\\n제5장: 결전의 순간\\n\\n미코와 료는 마침내 울프 코퍼레이션의 최상층에 도착해 CEO인 교활한 울프 박사와 대면한다. 울프 박사는 시민을 지배하려는 사악한 야망을 드러내며 자신의 압도적인 힘을 과시한다. 하지만 미코와 료는 서로를 도와가며 울프 박사와 싸우고 그의 약점을 찾아낸다.\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: 미코의 성격은?\\nAnswer: '}], 'model': 'gpt-3.5-turbo', 'n': 1, 'stream': False, 'temperature': 0.7}}\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7fea8cb153d0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x7fea68f23340> server_hostname='api.openai.com' timeout=None\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7fea8cb151c0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 27 Nov 2023 13:23:05 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-0613'), (b'openai-organization', b'user-lj8qeahdj7njstnyft5lsza9'), (b'openai-processing-ms', b'7870'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'200'), (b'x-ratelimit-limit-tokens', b'40000'), (b'x-ratelimit-limit-tokens_usage_based', b'40000'), (b'x-ratelimit-remaining-requests', b'199'), (b'x-ratelimit-remaining-tokens', b'39654'), (b'x-ratelimit-remaining-tokens_usage_based', b'39654'), (b'x-ratelimit-reset-requests', b'7m12s'), (b'x-ratelimit-reset-tokens', b'519ms'), (b'x-ratelimit-reset-tokens_usage_based', b'519ms'), (b'x-request-id', b'355b092330243e436b42acfe1517e447'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=EJ_G2kxiDVwSTJEhxf16Rm9xqj4EWZZHtWnxLqNU6fM-1701091385-0-ATpgrgVxD2uo9Pkzwe8rrm1IJzMCuZtMkpLCudFZs+rt5RHUtzGlnYr4dRoxYsRpzcB8FbN6VB0wn0StO1NUUfk=; path=/; expires=Mon, 27-Nov-23 13:53:05 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=osbzVmftAqAxu5_Q_C.M.gW_gpNVIVBXPy9L.2lY.yk-1701091385517-0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'82caaed508239326-ICN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Request: POST https://api.openai.com/v1/chat/completions \"200 OK\"\n",
      "DEBUG:llama_index.llm_predictor.base:미코는 울프 박사의 약점을 파고들어 그를 쓰러뜨리는데 성공하고, 해커 집단과 함께 울프 코퍼레이션의 악행을 공개하고 시민들을 해방시킨다. 이러한 행동들을 통해 미코는 용감하고 결단력이 강한 성격을 가지고 있을 것으로 추측할 수 있다.\n",
      "미코는 울프 박사의 약점을 파고들어 그를 쓰러뜨리는데 성공하고, 해커 집단과 함께 울프 코퍼레이션의 악행을 공개하고 시민들을 해방시킨다. 이러한 행동들을 통해 미코는 용감하고 결단력이 강한 성격을 가지고 있을 것으로 추측할 수 있다.\n"
     ]
    }
   ],
   "source": [
    "print(query_engine.query(\"미코의 성격은?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abc403ea728a4df19f86b45e8098b01a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:llama_index.indices.utils:> Top 2 nodes:\n",
      "> [Node 4] [Similarity score:             45.9988] 제5장: 결전의 순간\n",
      "\n",
      "미코와 료는 마침내 울프 코퍼레이션의 최상층에 도착해 CEO인 교활한 울프 박사와 대면한다. 울프 박사는 시민을 지배하려는 사악한 야망을 드러내며 자신...\n",
      "> [Node 1] [Similarity score:             47.3819] 제2장: 울프 코퍼레이션의 함정\n",
      "\n",
      "미코는 목적지인 술집 '할머니의 집'으로 향하는 길에 울프 코퍼레이션의 요원들에게 쫓기게 된다. 그들은 '빨간 망토'라는 데이터 카우리아에 ...\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': \"You are an expert Q&A system that is trusted around the world.\\nAlways answer the query using the provided context information, and not prior knowledge.\\nSome rules to follow:\\n1. Never directly reference the given context in your answer.\\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\"}, {'role': 'user', 'content': \"Context information is below.\\n---------------------\\nfile_path: data/akazukin5.txt\\n\\n제5장: 결전의 순간\\n\\n미코와 료는 마침내 울프 코퍼레이션의 최상층에 도착해 CEO인 교활한 울프 박사와 대면한다. 울프 박사는 시민을 지배하려는 사악한 야망을 드러내며 자신의 압도적인 힘을 과시한다. 하지만 미코와 료는 서로를 도와가며 울프 박사와 싸우고 그의 약점을 찾아낸다.\\n\\nfile_path: data/akazukin2.txt\\n\\n제2장: 울프 코퍼레이션의 함정\\n\\n미코는 목적지인 술집 '할머니의 집'으로 향하는 길에 울프 코퍼레이션의 요원들에게 쫓기게 된다. 그들은 '빨간 망토'라는 데이터 카우리아에 대한 소문을 듣고 데이터를 탈취하려 했다. 미코는 교묘하게 요원들을 흩뿌리고 술집에 도착한다.\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: 울프 코퍼레이션의 CEO의 이름은?\\nAnswer: \"}], 'model': 'gpt-3.5-turbo', 'n': 1, 'stream': False, 'temperature': 0.7}}\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 27 Nov 2023 13:23:11 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-0613'), (b'openai-organization', b'user-lj8qeahdj7njstnyft5lsza9'), (b'openai-processing-ms', b'3408'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'200'), (b'x-ratelimit-limit-tokens', b'40000'), (b'x-ratelimit-limit-tokens_usage_based', b'40000'), (b'x-ratelimit-remaining-requests', b'198'), (b'x-ratelimit-remaining-tokens', b'39641'), (b'x-ratelimit-remaining-tokens_usage_based', b'39641'), (b'x-ratelimit-reset-requests', b'14m13.234s'), (b'x-ratelimit-reset-tokens', b'538ms'), (b'x-ratelimit-reset-tokens_usage_based', b'538ms'), (b'x-request-id', b'59201e555e823fc76295f61302908eda'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'82caaf185b9d9326-ICN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Request: POST https://api.openai.com/v1/chat/completions \"200 OK\"\n",
      "DEBUG:llama_index.llm_predictor.base:울프 코퍼레이션의 CEO의 이름은 교활한 울프 박사입니다.\n",
      "울프 코퍼레이션의 CEO의 이름은 교활한 울프 박사입니다.\n"
     ]
    }
   ],
   "source": [
    "print(query_engine.query(\"울프 코퍼레이션의 CEO의 이름은?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama Hub\n",
    "\n",
    "- **위키피디아, 구글 드라이브, 구글 독스, gmail, 노션, spotify, youtube 등**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Youtube Transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.youtube.com:443\n",
      "DEBUG:urllib3.connectionpool:https://www.youtube.com:443 \"GET /watch?v=zPsiPvNod08 HTTP/1.1\" 200 None\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.youtube.com:443\n",
      "DEBUG:urllib3.connectionpool:https://www.youtube.com:443 \"GET /api/timedtext?v=zPsiPvNod08&ei=RphkZcP3D7ux1d8PqNGv-A8&caps=asr&opi=112496729&xoaf=5&hl=ko&ip=0.0.0.0&ipbits=0&expire=1701116598&sparams=ip,ipbits,expire,v,ei,caps,opi,xoaf&signature=4018A144474A52A385EF01E9F70F24510B2EACB5.10BB526B4594A72C7FCDBA18D81FA2442594BEDC&key=yt8&lang=en HTTP/1.1\" 200 None\n"
     ]
    }
   ],
   "source": [
    "from llama_index import download_loader\n",
    "\n",
    "YoutubeTranscriptReader = download_loader(\"YoutubeTranscriptReader\")\n",
    "loader = YoutubeTranscriptReader()\n",
    "yt_transcript = loader.load_data(ytlinks=[\"https://youtu.be/zPsiPvNod08?si=3pxE0QzthzMI_dEd\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='/Users/sean_forhim/opt/anaconda3/envs/llama_index/lib/python3.9/site-packages/certifi/cacert.pem'\n",
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='/Users/sean_forhim/opt/anaconda3/envs/llama_index/lib/python3.9/site-packages/certifi/cacert.pem'\n",
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='/Users/sean_forhim/opt/anaconda3/envs/llama_index/lib/python3.9/site-packages/certifi/cacert.pem'\n",
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='/Users/sean_forhim/opt/anaconda3/envs/llama_index/lib/python3.9/site-packages/certifi/cacert.pem'\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Oh, great, great\n",
      "I came to this place a lot\n",
      "wit...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Yes, yes, that, yes\n",
      "This guy is..\n",
      "from a star.....\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: All you need is a leg\n",
      "of a small octopus\n",
      "Leg of...\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x7fea69bccf70>, 'json_data': {'input': [\"video_id: zPsiPvNod08  Oh, great, great I came to this place a lot with other teachers The octopus here.. Don't even mention it It's not a joke I can see from this place What do you say.. this place has a Michelin vibe Michelin vibe The vibe! One Love Alpine Club V.. Vibe? There's a vibe Means power, power Power! God damn it! Damn - Oh, why? - Hello I said I wanted Baeksook *Korean chicken soup why are you keep coming to a place like this? No, that's not it... Small octopus here is... I said I wanted Baeksook! We just wanted something healthy for.. I said I wanted to Baeksook! Baeksook is when we go to mount.. I said I wanted to Baeksook! You need to eat something like small oc.. I said I wanted to Baeksook! There are other things that are goo.. I said I wanted Baeksook! You have only a few days left until your checkups, man God damn it I said I wanted Baeksook, but no one's listening to the leader All done, all done Just lower, lower, lower your voice Just don't be that Just.. cool down a bit.. okay? If you want something healthy, You need to eat Baeksook, I said Small octopus is also very good for your stamina You just try it Yeah, the sucker strength of small octopus is crazy so let's just try this healthy food together This is a total pain in the a** The small octopus place today, you know? we're all here today, and a teach Jeong Gwangyong is going to get us some nice healthy food Yes, yes Let's have a great time Who's going to order? It's me, right here Hehehehehehehehe What a stud you are Put a smile on your face, Leader Youngnam Yes, put a smile on your face I've always been telling you this but when the weather is getting hot like this this is when you sweat and lose your strength And we got only few days left until our health checkups This is when you have to treat yourself with good food so that our energies get fully charged Yes, yes, that's so true Get it? But when you're talking about a healthy food of course a small octopus is good but Baeksook is the one for it Get it? I said, we eat a lot of Baeksook when we go to mountain so let's eat what Mr. Jeong has recommended us today, okay? Small octopus You need to take care of your health real nice, you know That's right Tomato juice is very good for antioxidants Do.. Do you have tomato juice at.. Try these side dishes, these are great - Okay, okay, okay - Try it, try it, try it Try it, try it stop talking You... Try this, This Hey, try this, here There you go~ I mean, in the states Their eating habits are all about protein, you know Ohhhhh They have good strength but their big So you need to eat a lot of vegetables So, protein, carb, and fat you say P,C,F Carb... P,C,F Yes I only have fat here This dude.. You're talking nonsense again Eat up, eat up You.. are.. Try it, try it Oh, okay, okay, okay Eat up, eat up But if you look at health, the reason why I'm the most healthy is beCAUSE  I have a good habit of eathing health products I saw him eating like tens and dozens of pills Yes, I eat dozens of pills a day I know, yes But I'm going to recommend you the most crucial ones This is called Nutine Yes This is from a cousin of a friend in my hometown and this is a global brand Oh, that's a great relationship you have Yes, yes, yes Isn't Kim Soohyun a model of this brand? Yes, yes, that, yes This guy is.. from a star... *K-drama 'You came from the stars' Yes, that is correct Yes, yes, yes He's a model Oh, he's the model?\", \"video_id: zPsiPvNod08  Yes, yes, that, yes This guy is.. from a star... *K-drama 'You came from the stars' Yes, that is correct Yes, yes, yes He's a model Oh, he's the model? The nutrients you need not too much, not too little It's designed like that giving us the right nutritional balance That's what this is Wow, I love their slogan What's that? 'New habits to build health' 'My New Routine, Nutine' That's great, right? It means a New Routine Yes, yes, that's cool the meaning is awesome Now, what you do is before you eat, This is a vegetable rTG omega3 80 Vege... What? What? Vegetable Omega3 Vegetable Omega3! Try two pills each Yes? Try two pills each You eat this first? Eat this first, eat this first This, give me it I've seen this before This is good for your blood circulation and eyes It's a big help for them Blood? Blood? Blood circulation Blood circulation! Blood circulation Taekjo, you said your eyes weren't so great Yeah, my eyes were kind of blurry I couldn't really see well at night because I'm watching Youtube all the time Try this, eat this I just need to eat this? Yeah, try these two two pills at once Gwangyong is just chewing on them Wow This is the size is small it's going in real smooth into my throat - Really? - Right? The ones from the state are huge It's cause their throats are this wide It's really hard to swallow Yes, okay, let's eat now, let's eat Wow, look at this small octopus Isn't this a regular octopus? How fresh Look at its strength, look at it It looks like it's going to taste great You just need to feed one of this when a cow is knocked down back in the days You just slide one in their mouth and they just stood up That's right, that's right That stood right up That's why they were like 'Mooo' when they stood up like 'Mooo' 'Mooo' my a** All you need is a chicken I said I didn't want this This is really nice... I'd rather have a chicken leg instead of a leg of octopus Stop it, that's enough You can take Nutine for your nutrients Yeah, you can just eat them if you want to be healthy You can eat both Nutine and Baeksook Then you go eat that! I said I wanted Baeksook! Stop it I said I wanted Baeksook! Stop it I said I wanted Baeksook! Stop it Oh, for f**k sake f**king Baeksook... for the hundredth time We're good, we're good now Just order me one chicken for me Flipping heck, what a pain in the a** Who are you saying is a pain in the a**, damn What do you mean by pain in the a** wHaT dO yOu MeAn bY pAiN In tHe a** That's enough You gotta age nicely I will Woah.. Wow, this is great Oh, this is delightful Wow Wow.. Argh Damn Wow... This is delightful Wow What do you mean by a chicken leg? All you need is a leg of a small octopus Leg of octopus, leg of octopus It's a small octopus, small octopus Oh, yeah small octopus Leg of small octopus, small octopus This is the best Hoo.... Ahhhh Mmm~ Baeksook is so much better Why are you sweating so much? You try this... Mmm, this is good You eat them all Mmm, you try it, too You eat them all Slow down, slow down you Darn you Pour me some That's enough, that's enough Hey, that's enough I'm gonna get drunk Argh Argh, that was nice, right? It was tasty Whew, that was really nice Ah, it was great Yes, the small octopus is so delicious, right?\", \"video_id: zPsiPvNod08  All you need is a leg of a small octopus Leg of octopus, leg of octopus It's a small octopus, small octopus Oh, yeah small octopus Leg of small octopus, small octopus This is the best Hoo.... Ahhhh Mmm~ Baeksook is so much better Why are you sweating so much? You try this... Mmm, this is good You eat them all Mmm, you try it, too You eat them all Slow down, slow down you Darn you Pour me some That's enough, that's enough Hey, that's enough I'm gonna get drunk Argh Argh, that was nice, right? It was tasty Whew, that was really nice Ah, it was great Yes, the small octopus is so delicious, right? I told you it was good Let's go, let's go There are still some octopus left there Thank you for the meal Thank you so much Thank you so very much, right? Yes, that's right That was great Ahh, how nice My tummy is full What fancy thing are you eating by yourself? Give me some Damn it, you're.. Try it What is this It's called Tart Cherry Collagen This is bloody good for you Eat stuff like this instead of coffee you take care of your body with things like this Ahh, but this is it'd be perfect if you eat this as a snack - Right? - Yes This is 100% american tart cherry But this is a bit sour for me This cannot not be sour because The fruit content is very very high so that's why the taste is sour like this You need to deal with it Ahh, it's so nice to have this after small octopus What do you mean it's good to have this with small octopus? It will be so much better with Beaksook Hey, we can't eat this with Baeksook everyday It's so much better with Baeksook! It's so much better with Baeksook! Cut it out, cut it out now - It's so much better with Baeksook! - Stop it, stop it, Youngnam You stop it, you had enough today What do you mean you had enough? You need to know the line for god sake! We just had small octopu... Octopus... If you keep yelling in a public place, people are looking at us Did you just say people are looking, you bastard? Who are you calling a bastard, man? We're all over 60 years old If you keep on saying that just because you're the leader What did you just say right now? I said stop it, you! Who are you yelling at? St.. Stop it... That's enough, enough What's wrong with that dude? You need to know the limit! You're the one with weird personality - Enough, enough, enough - Cut it out Enough, enough, enough You stop living like that right now Do you know how much I'm praying for you? Enough, enough One Love Alpine Club Enough, enough I mean, he's the one who did wrong... - Hey, hey! - This isn't the first ti... I get it, I get it This isn't the first time, you know I totally agree with you But still, think about it, He should be leading us.. Ayyy, you're the best Mr. Bae is the best, the best If he does this one more time, I'm.. Hey! I won't forget this Mr. Bae There, there I'm fine now! Okay, okay There, there Hehehehe This video contains paid advertisements of Nutine\"], 'model': <OpenAIEmbeddingModeModel.TEXT_EMBED_ADA_002: 'text-embedding-ada-002'>, 'encoding_format': 'base64'}}\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=60.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7fea69c06520>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x7fea299e8840> server_hostname='api.openai.com' timeout=60.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7fea69c064f0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 27 Nov 2023 13:23:21 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'user-lj8qeahdj7njstnyft5lsza9'), (b'openai-processing-ms', b'57'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'200'), (b'x-ratelimit-limit-tokens', b'150000'), (b'x-ratelimit-remaining-requests', b'199'), (b'x-ratelimit-remaining-tokens', b'147590'), (b'x-ratelimit-reset-requests', b'7m12s'), (b'x-ratelimit-reset-tokens', b'964ms'), (b'x-request-id', b'5984c492d74f2d93700c9c1953c33844'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=rr0Q2VT8Lfujid4IcXNFN1U3ONVQ7XJo5JP_p7ziH38-1701091401-0-AXqi28szvoh7ystUNGxaw7QKA3zOd3PxLNq/2jiyERlm06meYvtPZ/ZIhn344PEqi3rsdS8ezQMlF5PMb8EWrng=; path=/; expires=Mon, 27-Nov-23 13:53:21 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=ZgdiBMmUxNNIRDMrYlP_fWTzw0sV8fnQHwoxFLQLAnU-1701091401228-0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'82caaf67bed9931d-ICN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Request: POST https://api.openai.com/v1/embeddings \"200 OK\"\n"
     ]
    }
   ],
   "source": [
    "index1 = GPTVectorStoreIndex.from_documents(yt_transcript)\n",
    "query_engine1 = index1.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x7fea69bf3820>, 'json_data': {'input': ['이 동영상에서 민수가 가장 먹고 싶어 하는 음식이 무엇인가요?'], 'model': <OpenAIEmbeddingModeModel.TEXT_EMBED_ADA_002: 'text-embedding-ada-002'>, 'encoding_format': 'base64'}}\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 27 Nov 2023 13:23:22 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'user-lj8qeahdj7njstnyft5lsza9'), (b'openai-processing-ms', b'18'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'200'), (b'x-ratelimit-limit-tokens', b'150000'), (b'x-ratelimit-remaining-requests', b'198'), (b'x-ratelimit-remaining-tokens', b'149979'), (b'x-ratelimit-reset-requests', b'14m22.8s'), (b'x-ratelimit-reset-tokens', b'8ms'), (b'x-request-id', b'1ae7d75465bf6c3dfeb1c77264fb7433'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'82caaf6f0ce5931d-ICN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Request: POST https://api.openai.com/v1/embeddings \"200 OK\"\n",
      "DEBUG:llama_index.indices.utils:> Top 2 nodes:\n",
      "> [Node 5a1187ed-515d-465a-8c94-200f982a002a] [Similarity score:             0.788616] Yes, yes, that, yes\n",
      "This guy is..\n",
      "from a star...\n",
      "*K-drama 'You came from the stars'\n",
      "Yes, that is ...\n",
      "> [Node e4f13ae5-6c88-4837-af56-1c672d38501c] [Similarity score:             0.769485] Oh, great, great\n",
      "I came to this place a lot\n",
      "with other teachers\n",
      "The octopus here..\n",
      "Don't even men...\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': <MessageRole.SYSTEM: 'system'>, 'content': \"You are an expert Q&A system that is trusted around the world.\\nAlways answer the query using the provided context information, and not prior knowledge.\\nSome rules to follow:\\n1. Never directly reference the given context in your answer.\\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\"}, {'role': <MessageRole.USER: 'user'>, 'content': \"Context information is below.\\n---------------------\\nvideo_id: zPsiPvNod08\\n\\nYes, yes, that, yes\\nThis guy is..\\nfrom a star...\\n*K-drama 'You came from the stars'\\nYes, that is correct\\nYes, yes, yes\\nHe's a model\\nOh, he's the model?\\nThe nutrients you need\\nnot too much,\\nnot too little\\nIt's designed like that\\ngiving us\\nthe right nutritional balance\\nThat's what this is\\nWow, I love their slogan\\nWhat's that?\\n'New habits to build health'\\n'My New Routine, Nutine'\\nThat's great, right?\\nIt means a New Routine\\nYes, yes, that's cool\\nthe meaning is awesome\\nNow, what you do is\\nbefore you eat,\\nThis is a vegetable\\nrTG omega3 80\\nVege...\\nWhat? What?\\nVegetable Omega3\\nVegetable Omega3!\\nTry two pills each\\nYes? Try two pills each\\nYou eat this first?\\nEat this first,\\neat this first\\nThis, give me it\\nI've seen this before\\nThis is good for your\\nblood circulation and eyes\\nIt's a big help for them\\nBlood? Blood?\\nBlood circulation\\nBlood circulation!\\nBlood circulation\\nTaekjo, you said\\nyour eyes weren't so great\\nYeah, my eyes were\\nkind of blurry\\nI couldn't really\\nsee well at night\\nbecause I'm watching\\nYoutube all the time\\nTry this, eat this\\nI just need to eat this?\\nYeah, try these two\\ntwo pills at once\\nGwangyong is just\\nchewing on them\\nWow\\nThis is\\nthe size is small\\nit's going in real smooth\\ninto my throat\\n- Really?\\n- Right?\\nThe ones from\\nthe state are huge\\nIt's cause their\\nthroats are this wide\\nIt's really hard to swallow\\nYes, okay,\\nlet's eat now, let's eat\\nWow, look at\\nthis small octopus\\nIsn't this a regular octopus?\\nHow fresh\\nLook at its strength,\\nlook at it\\nIt looks like\\nit's going to taste great\\nYou just need to feed\\none of this when\\na cow is knocked down\\nback in the days\\nYou just slide one\\nin their mouth\\nand they just stood up\\nThat's right,\\nthat's right\\nThat stood right up\\nThat's why\\nthey were like 'Mooo'\\nwhen they stood up\\nlike 'Mooo'\\n'Mooo' my a**\\nAll you need\\nis a chicken\\nI said I didn't want this\\nThis is really nice...\\nI'd rather have a chicken leg\\ninstead of a leg of octopus\\nStop it, that's enough\\nYou can take Nutine\\nfor your nutrients\\nYeah, you can just\\neat them if you want to be healthy\\nYou can eat both\\nNutine and Baeksook\\nThen you go eat that!\\nI said I wanted Baeksook!\\nStop it\\nI said I wanted Baeksook!\\nStop it\\nI said I wanted Baeksook!\\nStop it\\nOh, for f**k sake\\nf**king Baeksook...\\nfor the hundredth time\\nWe're good,\\nwe're good now\\nJust order me\\none chicken for me\\nFlipping heck,\\nwhat a pain in the a**\\nWho are you saying\\nis a pain in the a**, damn\\nWhat do you mean\\nby pain in the a**\\nwHaT dO yOu MeAn\\nbY pAiN In tHe a**\\nThat's enough\\nYou gotta age nicely\\nI will\\nWoah..\\nWow, this is great\\nOh, this is delightful\\nWow\\nWow..\\nArgh\\nDamn\\nWow...\\nThis is delightful\\nWow\\nWhat do you mean\\nby a chicken leg?\\nAll you need is a leg\\nof a small octopus\\nLeg of octopus,\\nleg of octopus\\nIt's a small octopus,\\nsmall octopus\\nOh, yeah\\nsmall octopus\\nLeg of small octopus,\\nsmall octopus\\nThis is the best\\nHoo....\\nAhhhh\\nMmm~\\nBaeksook is so much better\\nWhy are you sweating so much?\\nYou try this...\\nMmm, this is good\\nYou eat them all\\nMmm,\\nyou try it, too\\nYou eat them all\\nSlow down,\\nslow down you\\nDarn you\\nPour me some\\nThat's enough,\\nthat's enough\\nHey, that's enough\\nI'm gonna get drunk\\nArgh\\nArgh, that was nice, right?\\nIt was tasty\\nWhew, that was\\nreally nice\\nAh, it was great\\nYes, the small octopus is\\nso delicious, right?\\n\\nvideo_id: zPsiPvNod08\\n\\nOh, great, great\\nI came to this place a lot\\nwith other teachers\\nThe octopus here..\\nDon't even mention it\\nIt's not a joke\\nI can see from this place\\nWhat do you say..\\nthis place\\nhas a Michelin vibe\\nMichelin vibe\\nThe vibe!\\nOne Love Alpine Club\\nV.. Vibe?\\nThere's a vibe\\nMeans power, power\\nPower!\\nGod damn it!\\nDamn\\n- Oh, why?\\n- Hello\\nI said I wanted Baeksook\\n*Korean chicken soup\\nwhy are you keep\\ncoming to a place like this?\\nNo, that's not it...\\nSmall octopus here is...\\nI said I wanted Baeksook!\\nWe just wanted\\nsomething healthy for..\\nI said I wanted to Baeksook!\\nBaeksook is\\nwhen we go to mount..\\nI said I wanted to Baeksook!\\nYou need to eat\\nsomething like small oc..\\nI said I wanted to Baeksook!\\nThere are other things\\nthat are goo..\\nI said I wanted Baeksook!\\nYou have only a few days\\nleft until your checkups, man\\nGod damn it\\nI said I wanted Baeksook,\\nbut no one's listening to the leader\\nAll done,\\nall done\\nJust lower,\\nlower, lower your voice\\nJust don't be that\\nJust.. cool down a bit..\\nokay?\\nIf you want\\nsomething healthy,\\nYou need to\\neat Baeksook, I said\\nSmall octopus is also\\nvery good for your stamina\\nYou just try it\\nYeah, the sucker strength\\nof small octopus is crazy\\nso let's just try this\\nhealthy food together\\nThis is a total pain in the a**\\nThe small octopus\\nplace today, you know?\\nwe're all here today,\\nand a teach Jeong Gwangyong\\nis going to get us\\nsome nice healthy food\\nYes, yes\\nLet's have a great time\\nWho's going to order?\\nIt's me, right here\\nHehehehehehehehe\\nWhat a stud you are\\nPut a smile on your face,\\nLeader Youngnam\\nYes, put a smile\\non your face\\nI've always been\\ntelling you this but\\nwhen the weather\\nis getting hot like this\\nthis is when you sweat\\nand lose your strength\\nAnd we got only few days left\\nuntil our health checkups\\nThis is when you have to\\ntreat yourself with good food\\nso that our energies\\nget fully charged\\nYes, yes,\\nthat's so true\\nGet it?\\nBut when you're talking\\nabout a healthy food\\nof course\\na small octopus is good\\nbut Baeksook is the one for it\\nGet it?\\nI said, we eat a lot of Baeksook\\nwhen we go to mountain\\nso let's eat what Mr. Jeong\\nhas recommended us today, okay?\\nSmall octopus\\nYou need to take care of\\nyour health real nice, you know\\nThat's right\\nTomato juice is\\nvery good for antioxidants\\nDo.. Do you have\\ntomato juice at..\\nTry these side dishes,\\nthese are great\\n- Okay, okay, okay\\n- Try it, try it, try it\\nTry it, try it\\nstop talking\\nYou...\\nTry this, This\\nHey, try this, here\\nThere you go~\\nI mean, in the states\\nTheir eating habits are\\nall about protein, you know\\nOhhhhh\\nThey have good strength\\nbut their big\\nSo you need to eat\\na lot of vegetables\\nSo, protein, carb, and fat\\nyou say P,C,F\\nCarb...\\nP,C,F\\nYes\\nI only have fat here\\nThis dude..\\nYou're talking nonsense again\\nEat up, eat up\\nYou.. are..\\nTry it, try it\\nOh, okay, okay, okay\\nEat up, eat up\\nBut if you look at health,\\nthe reason why I'm\\nthe most healthy\\nis beCAUSE\\n I have a good habit\\nof eathing health products\\nI saw him eating like\\ntens and dozens of pills\\nYes, I eat dozens of pills a day\\nI know, yes\\nBut I'm going to recommend\\nyou the most crucial ones\\nThis is called Nutine\\nYes\\nThis is from\\na cousin of a friend\\nin my hometown\\nand this is a global brand\\nOh, that's a great\\nrelationship you have\\nYes, yes, yes\\nIsn't Kim Soohyun\\na model of this brand?\\nYes, yes, that, yes\\nThis guy is..\\nfrom a star...\\n*K-drama 'You came from the stars'\\nYes, that is correct\\nYes, yes, yes\\nHe's a model\\nOh, he's the model?\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: 이 동영상에서 민수가 가장 먹고 싶어 하는 음식이 무엇인가요?\\nAnswer: \"}], 'model': 'gpt-3.5-turbo', 'stream': False, 'temperature': 0.1}}\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=60.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7fea080d5ac0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x7fea299e8640> server_hostname='api.openai.com' timeout=60.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7fea080d5a90>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 27 Nov 2023 13:23:25 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-0613'), (b'openai-organization', b'user-lj8qeahdj7njstnyft5lsza9'), (b'openai-processing-ms', b'3124'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'200'), (b'x-ratelimit-limit-tokens', b'40000'), (b'x-ratelimit-limit-tokens_usage_based', b'40000'), (b'x-ratelimit-remaining-requests', b'197'), (b'x-ratelimit-remaining-tokens', b'38157'), (b'x-ratelimit-remaining-tokens_usage_based', b'38157'), (b'x-ratelimit-reset-requests', b'21m10.899s'), (b'x-ratelimit-reset-tokens', b'2.764s'), (b'x-ratelimit-reset-tokens_usage_based', b'2.764s'), (b'x-request-id', b'6cac62c6cbfc14a93686ec46806ebadc'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=d3rlDJcQdULqv8licx5142MhZ1Xc3OFpx5aJd8IrLP8-1701091405-0-Adcmasq8wcPsYFtOR/r7zv95r5LYUndv/bSXgstE5kPryI95rR7r0ra2b4zCgKg7u+IflGyuoThAq3xMrOksVgc=; path=/; expires=Mon, 27-Nov-23 13:53:25 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=2ee4umbDAC5gyg1glmVY0H.DimOKPgjtRvhjRKC8NY0-1701091405906-0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'82caaf71bfbdedad-ICN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Request: POST https://api.openai.com/v1/chat/completions \"200 OK\"\n",
      "DEBUG:llama_index.llm_predictor.base:민수가 가장 먹고 싶어하는 음식은 백숙입니다.\n",
      "민수가 가장 먹고 싶어하는 음식은 백숙입니다.\n"
     ]
    }
   ],
   "source": [
    "print(query_engine1.query(\"이 동영상에서 민수가 가장 먹고 싶어 하는 음식이 무엇인가요?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
