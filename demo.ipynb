{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama Index Demo 및 발표\n",
    "### 디지털애널리틱스융합협동과정 2023311??? 최민철 <br> 디지털애널리틱스융합협동과정 2023311561 이정환"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Knowledge\n",
    "LLM에서 학습되지 않은 데이터를 참조해서 질의응답(채팅)을 작성하기 위한 애플리케이션.\n",
    "추가 학습 없이 응답 시 해당 정보를 참조하는 구조로, 가볍고 쉽게 구축 및 처리가 가능함.\n",
    "학습되지 않은 데이터라면, 모델 학습 이후의 데이터 혹은 공개되지 않은 사내 데이터 등이 될 수 있음.\n",
    "해당 데이터를 바탕으로 답변과 관련된 정보를 검색해서 입력 프롬프트에 삽입, LLM의 추론 능력을 활용해 응답을 생성함.\n",
    "텍스트뿐만 아니라 PDF 등의 다양한 파일형식이나 Youtube와 같은 웹 서비스 정보도 질의응답에 활용할 수 있음.\n",
    "\n",
    "## RAG vs FineTuning\n",
    "![rag_vs_finetuning.png](./image/rag_vs_finetuning.png)\n",
    "\n",
    "## RAG 5 단계\n",
    "![rag_five_stage.png](./image/rag_five_stage.png)\n",
    "\n",
    "- **로딩(Loading)**: 데이터를 소스(텍스트 파일, PDF, 웹사이트, 데이터베이스, API 등)에서 파이프라인으로 가져오는 단계입니다. LlamaHub는 다양한 커넥터를 제공합니다.\n",
    "- **인덱싱(Indexing)**: 데이터 쿼리를 가능하게 하는 데이터 구조를 생성하는 과정입니다. LLM에서는 주로 벡터 임베딩(데이터의 의미를 나타내는 숫자 표현)을 생성합니다. 또한, 문맥적으로 관련된 데이터를 정확하게 찾기 쉽게 하기 위한 다양한 메타데이터 전략도 사용됩니다.\n",
    "- **저장(Storing)**: 데이터를 인덱싱한 후에는, 다시 인덱싱할 필요를 피하기 위해 인덱스와 다른 메타데이터를 저장합니다.\n",
    "- **쿼리(Querying)**: 주어진 인덱싱 전략에 따라 LLM과 LlamaIndex 데이터 구조를 사용하여 다양한 방식으로 쿼리를 수행할 수 있습니다. 이에는 하위 쿼리, 다단계 쿼리, 하이브리드 전략 등이 포함됩니다.\n",
    "- **평가(Evaluation)**: 파이프라인의 효과성을 다른 전략과 비교하거나 변경사항이 있을 때 체크하는 중요한 단계입니다. 평가는 쿼리에 대한 응답의 정확성, 충실도, 속도를 객관적으로 측정합니다.\n",
    "2. ****Important concepts within each step****\n",
    "    \n",
    "    LlamaIndex의 각 단계에서 중요한 개념들은 다음과 같습니다:\n",
    "    \n",
    "    - **로딩 단계**\n",
    "        - **노드와 문서**: 문서는 데이터 소스(예: PDF, API 출력, 데이터베이스에서 검색된 데이터)를 둘러싼 컨테이너입니다. 노드는 LlamaIndex에서 데이터의 기본 단위로, 소스 문서의 \"청크\"를 나타냅니다. 노드에는 해당 문서와 다른 노드와의 관계를 설명하는 메타데이터가 있습니다.\n",
    "        - **커넥터**: 데이터 커넥터(리더라고도 함)는 다양한 데이터 소스와 데이터 형식에서 데이터를 문서와 노드로 가져옵니다.\n",
    "    - **인덱싱 단계**\n",
    "        - **인덱스**: 데이터를 수집한 후, LlamaIndex는 검색하기 쉬운 구조로 데이터를 인덱싱하는 데 도움을 줍니다. 이는 주로 벡터 임베딩 생성을 포함하며, 이들은 벡터 저장소라는 전문 데이터베이스에 저장됩니다. 인덱스는 데이터에 대한 다양한 메타데이터도 저장할 수 있습니다.\n",
    "        - **임베딩**: LLM은 데이터의 수치적 표현인 임베딩을 생성합니다. LlamaIndex는 쿼리를 임베딩으로 변환하고, 벡터 저장소는 쿼리의 임베딩과 수치적으로 유사한 데이터를 찾습니다.\n",
    "    - **쿼리 단계**\n",
    "        - **검색기**: 검색기는 쿼리가 주어졌을 때 인덱스에서 관련 컨텍스트를 효율적으로 검색하는 방법을 정의합니다. 검색 전략은 검색된 데이터의 관련성과 효율성에 중요합니다.\n",
    "        - **라우터**: 라우터는 지식 기반에서 관련 컨텍스트를 검색하는 데 사용될 검색기를 결정합니다. 구체적으로, RouterRetriever 클래스는 하나 이상의 후보 검색기를 선택하고, 각 후보의 메타데이터와 쿼리에 기반하여 최적의 옵션을 선택합니다.\n",
    "        - **노드 후처리기**: 노드 후처리기는 검색된 노드 집합을 가져와 변환, 필터링 또는 재순위 결정 로직을 적용합니다.\n",
    "        - **응답 합성기**: 응답 합성기는 사용자 쿼리와 검색된 텍스트 청크 집합을 사용하여 LLM에서 응답을 생성합니다.\n",
    "        \n",
    "3. **종합적인 사용 사례**\n",
    "    - **쿼리 엔진**: 쿼리 엔진은 데이터에 대해 질문을 할 수 있는 엔드 투 엔드 파이프라인입니다. 자연어 쿼리를 받아 응답을 반환하며, 검색된 참조 컨텍스트를 LLM에 전달합니다.\n",
    "    - **채팅 엔진**: 채팅 엔진은 데이터와의 대화(단일 질문과 답변이 아닌 여러 차례 왕복)를 위한 엔드 투 엔드 파이프라인입니다.\n",
    "    - **에이전트**: 에이전트는 LLM으로 구동되는 자동화된 의사 결정자로, 다양한 도구를 통해 외부 세계와 상호 작용합니다. 에이전트는 주어진 작업을 완료하기 위해 임의의 수의 단계를 수행하며, 사전에 정해진 단계를"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# requirements settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone \n",
    "# %pip install poetry\n",
    "# %cd llama_index\n",
    "# %poetry install\n",
    "# %pip install -U llama-index\n",
    "# %pip install -U langchain\n",
    "# %pip install sentence-transformers\n",
    "# %pip install accelerate\n",
    "# %pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "# %pip install transformers==4.34.0\n",
    "# %pip install faiss-cpu\n",
    "# %pip install lamma-hub\n",
    "# %pip install youtube_transcript_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#로그 설정 & API Key 설정\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "import os \n",
    "\n",
    "# load .env\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG, force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 로드\n",
    "\n",
    "사용하고 싶은 데이터(문서) 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:llama_index.readers.file.base:> [SimpleDirectoryReader] Total files added: 7\n"
     ]
    }
   ],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM\n",
    "\n",
    "기본은 OpenAI의 chatgpt text-davinci-003 모델 설정. gpt-3.5-turbo 등도 사용 가능.\n",
    "custom model 사용 가능함. huggingface의 transformers 사용하여 custom class 생성 사용 혹은 langchain의 HuggingFaceLLM을 바로 사용 가능함. 우리는 LLM 특성 상 많은 리소스를 필요로 하지만, 리소스의 한계로 인해 chatgpt api를 활용함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='/Users/sean_forhim/opt/anaconda3/envs/llama_index/lib/python3.9/site-packages/certifi/cacert.pem'\n",
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='/Users/sean_forhim/opt/anaconda3/envs/llama_index/lib/python3.9/site-packages/certifi/cacert.pem'\n"
     ]
    }
   ],
   "source": [
    "from llama_index import LLMPredictor\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0.7, model_name=\"gpt-3.5-turbo\"))\n",
    "\n",
    "\n",
    "\n",
    "# from langchain.llms.base import LLM\n",
    "# from typing import Optional, List, Mapping, Any\n",
    "# from transformers import pipeline\n",
    "\n",
    "# # setting\n",
    "# context_window = 2048\n",
    "# num_output = 256\n",
    "# model_name = 'beomi/KoAlpaca-llama-1-7b'\n",
    "# pipe = pipeline(\"text-generation\", model=model_name)\n",
    "\n",
    "# class CustomLLM(LLM):\n",
    "#     def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "#         prompt_length = len(prompt)\n",
    "#         response = pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
    "\n",
    "#         # only return newly generated tokens\n",
    "#         return response[prompt_length:]\n",
    "\n",
    "#     @property\n",
    "#     def _identifying_params(self) -> Mapping[str, Any]:\n",
    "#         return {\"name_of_model\": model_name}\n",
    "\n",
    "#     @property\n",
    "#     def _llm_type(self) -> str:\n",
    "#         return \"custom\"\n",
    "\n",
    "    \n",
    "# # define our LLM\n",
    "# llm_predictor = LLMPredictor(llm=CustomLLM())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding\n",
    "\n",
    "기본 임베딩 모델은 OpenAI API의 text-embedding-ada-002가 기본으로 사용됨.\n",
    "역시 custom model, huggingface 모델이 사용 가능하며, 이번에는 HuggingFaceEmbeddings를 활용하여 huggingface 모델을 사용해봄."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: bongsoo/moco-sentencedistilbertV2.1\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device: cpu\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from llama_index.embeddings import LangchainEmbedding\n",
    "\n",
    "\n",
    "embed_model = LangchainEmbedding(HuggingFaceEmbeddings(model_name='bongsoo/moco-sentencedistilbertV2.1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataBase\n",
    "기본으로 제공되는 파이썬의 딕셔너리로 데이터를 보관하는 간단한 인덱스인 GPTVectorStoreIndex를 사용할 수 있음.\n",
    "이외에도 Faiss, Qdrant, Chroma, Milvus (이상 패키지로 제공하는 데이터베이스), Pinecone, Weaviate (이상 클라우드 서비스로 제공하는 데이터베이스) 총 여섯 가지의 벡터 데이터베이스를 사용할 수 있음. 우리는 무료로 일정부분 사용할 수 있는 Faiss 활용.\n",
    "벡터 데이터베이스를 사용할 경우, 빠른 검색과 높은 정확도, 확장성을 구현하여 자연어 등 대규모 데이터셋을 효율적으로 처리할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:faiss.loader:Loading faiss.\n",
      "INFO:faiss.loader:Successfully loaded faiss.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "import faiss\n",
    "\n",
    "\n",
    "# faiss의 indexing : IndexFlatL2, IndexFlatIP, IndexIVFFlat\n",
    "faiss_index = faiss.IndexFlatL2(768)\n",
    "vector_store = FaissVectorStore(faiss_index=faiss_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index\n",
    "라마인덱스의 종류에는 네 가지가 있음.\n",
    "\n",
    "## (1) 벡터스토어인덱스\n",
    "<img src=\"./image/vector_store.png\" width=\"500\"/>\n",
    "<img src=\"./image/vector_store_query.png\" width=\"500\"/>\n",
    "\n",
    "## (2) 리스트 인덱스\n",
    "<img src=\"./image/list.png\" width=\"500\"/>\n",
    "<img src=\"./image/list_query.png\" width=\"500\"/>\n",
    "\n",
    "## (3) 트리 인덱스\n",
    "<img src=\"./image/tree.png\" width=\"500\"/>\n",
    "<img src=\"./image/tree_query.png\" width=\"500\"/>\n",
    "\n",
    "\n",
    "## (4) 키워드 테이블 인덱스\n",
    "<img src=\"./image/keyword.png\" width=\"500\"/>\n",
    "<img src=\"./image/keyword_query.png\" width=\"500\"/>\n",
    "\n",
    "\n",
    "우리는 다른 벡터 데이터베이스를 활용한 인덱스 활용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: 제1장: 데이터 프론트\n",
      "\n",
      "밤이 되면 반짝이는 네오 도쿄. 고층 빌딩이 늘어서고, 네온...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: 제2장: 울프 코퍼레이션의 함정\n",
      "\n",
      "미코는 목적지인 술집 '할머니의 집'으로 향하는 길...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: 제3장: 배신과 재회\n",
      "\n",
      "술집 '할머니의 집'에서 미코는 데이터를 받을 사람인 료를 기...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: 제4장: 울프 코퍼레이션의 붕괴\n",
      "\n",
      "미코와 료는 해커 집단과 함께 울프 코퍼레이션에 대...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: 제5장: 결전의 순간\n",
      "\n",
      "미코와 료는 마침내 울프 코퍼레이션의 최상층에 도착해 CEO인...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: 제6장: 진실의 해방\n",
      "\n",
      "미코는 울프 박사의 약점을 파고들어 그를 쓰러뜨리는데 성공한다...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: 제7장: 새로운 시작\n",
      "\n",
      "울프 코퍼레이션이 무너진 후, 미코와 료는 서로의 과거를 용서...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bd449a51a2d41fca76b4cdd21de91d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index import GPTVectorStoreIndex, ServiceContext, StorageContext\n",
    "\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor,embed_model=embed_model)\n",
    "index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Engine\n",
    "쿼리 엔진은 사용자 입력과 관련된 정보를 인덱스에서 가져오고, 사용자 입력과 가져온 정보를 바탕으로 응답을 생성함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "849f7121b61e4d2196c1c902c39141bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:llama_index.indices.utils:> Top 2 nodes:\n",
      "> [Node 5] [Similarity score:             47.2367] 제6장: 진실의 해방\n",
      "\n",
      "미코는 울프 박사의 약점을 파고들어 그를 쓰러뜨리는데 성공한다. 그리고 해커 집단과 함께 울프 코퍼레이션의 악행을 세상에 공개하고 시민들을 해방시킨다....\n",
      "> [Node 4] [Similarity score:             48.4738] 제5장: 결전의 순간\n",
      "\n",
      "미코와 료는 마침내 울프 코퍼레이션의 최상층에 도착해 CEO인 교활한 울프 박사와 대면한다. 울프 박사는 시민을 지배하려는 사악한 야망을 드러내며 자신...\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': \"You are an expert Q&A system that is trusted around the world.\\nAlways answer the query using the provided context information, and not prior knowledge.\\nSome rules to follow:\\n1. Never directly reference the given context in your answer.\\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\"}, {'role': 'user', 'content': 'Context information is below.\\n---------------------\\nfile_path: data/akazukin6.txt\\n\\n제6장: 진실의 해방\\n\\n미코는 울프 박사의 약점을 파고들어 그를 쓰러뜨리는데 성공한다. 그리고 해커 집단과 함께 울프 코퍼레이션의 악행을 세상에 공개하고 시민들을 해방시킨다. 이 승리로 미코의 어머니의 치료법도 찾아내고, 그녀의 병은 완치된다.\\n\\nfile_path: data/akazukin5.txt\\n\\n제5장: 결전의 순간\\n\\n미코와 료는 마침내 울프 코퍼레이션의 최상층에 도착해 CEO인 교활한 울프 박사와 대면한다. 울프 박사는 시민을 지배하려는 사악한 야망을 드러내며 자신의 압도적인 힘을 과시한다. 하지만 미코와 료는 서로를 도와가며 울프 박사와 싸우고 그의 약점을 찾아낸다.\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: 미코의 성격은?\\nAnswer: '}], 'model': 'gpt-3.5-turbo', 'n': 1, 'stream': False, 'temperature': 0.7}}\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7fb867c3f040>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x7fb84137a6c0> server_hostname='api.openai.com' timeout=None\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7fb867c30b20>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 26 Nov 2023 13:02:14 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-0613'), (b'openai-organization', b'user-lj8qeahdj7njstnyft5lsza9'), (b'openai-processing-ms', b'15901'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'200'), (b'x-ratelimit-limit-tokens', b'40000'), (b'x-ratelimit-limit-tokens_usage_based', b'40000'), (b'x-ratelimit-remaining-requests', b'197'), (b'x-ratelimit-remaining-tokens', b'39654'), (b'x-ratelimit-remaining-tokens_usage_based', b'39654'), (b'x-ratelimit-reset-requests', b'16m6.119s'), (b'x-ratelimit-reset-tokens', b'519ms'), (b'x-ratelimit-reset-tokens_usage_based', b'519ms'), (b'x-request-id', b'a5f73d12b49e73190ea85b91023084ae'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=5KccKAuYmoRKgFe.iKInUdlir6E6rJRbOtjEGIJ1LtU-1701003734-0-AX6x6DE6I1iyTCmziBXd1duKM3BZnoMk3se/d8D/cc7miDF++vwWiNTh93vw9vVJL7ePBDfgRz3dxz3m2hOvnuw=; path=/; expires=Sun, 26-Nov-23 13:32:14 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=qnTjm3AndRTTAK9sFwEfjieGozgQ_G4hQGtNZ_ZsXi4-1701003734940-0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'82c252ba0d5cedd5-ICN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Request: POST https://api.openai.com/v1/chat/completions \"200 OK\"\n",
      "DEBUG:llama_index.llm_predictor.base:미코의 성격은 울프 박사의 약점을 파고들어 그를 쓰러뜨리는 데 성공하고, 해커 집단과 함께 울프 코퍼레이션의 악행을 공개하여 시민들을 해방시키는 용기와 결단력이 있는 것으로 보입니다. 그녀는 또한 료와 함께 울프 박사와 싸우며 그의 약점을 찾아내는 능력을 보여주고 있습니다.\n",
      "미코의 성격은 울프 박사의 약점을 파고들어 그를 쓰러뜨리는 데 성공하고, 해커 집단과 함께 울프 코퍼레이션의 악행을 공개하여 시민들을 해방시키는 용기와 결단력이 있는 것으로 보입니다. 그녀는 또한 료와 함께 울프 박사와 싸우며 그의 약점을 찾아내는 능력을 보여주고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "print(query_engine.query(\"미코의 성격은?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f777c0f87d7c4ab7ad13d04691d5d67e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:llama_index.indices.utils:> Top 2 nodes:\n",
      "> [Node 4] [Similarity score:             45.9988] 제5장: 결전의 순간\n",
      "\n",
      "미코와 료는 마침내 울프 코퍼레이션의 최상층에 도착해 CEO인 교활한 울프 박사와 대면한다. 울프 박사는 시민을 지배하려는 사악한 야망을 드러내며 자신...\n",
      "> [Node 1] [Similarity score:             47.3819] 제2장: 울프 코퍼레이션의 함정\n",
      "\n",
      "미코는 목적지인 술집 '할머니의 집'으로 향하는 길에 울프 코퍼레이션의 요원들에게 쫓기게 된다. 그들은 '빨간 망토'라는 데이터 카우리아에 ...\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': \"You are an expert Q&A system that is trusted around the world.\\nAlways answer the query using the provided context information, and not prior knowledge.\\nSome rules to follow:\\n1. Never directly reference the given context in your answer.\\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\"}, {'role': 'user', 'content': \"Context information is below.\\n---------------------\\nfile_path: data/akazukin5.txt\\n\\n제5장: 결전의 순간\\n\\n미코와 료는 마침내 울프 코퍼레이션의 최상층에 도착해 CEO인 교활한 울프 박사와 대면한다. 울프 박사는 시민을 지배하려는 사악한 야망을 드러내며 자신의 압도적인 힘을 과시한다. 하지만 미코와 료는 서로를 도와가며 울프 박사와 싸우고 그의 약점을 찾아낸다.\\n\\nfile_path: data/akazukin2.txt\\n\\n제2장: 울프 코퍼레이션의 함정\\n\\n미코는 목적지인 술집 '할머니의 집'으로 향하는 길에 울프 코퍼레이션의 요원들에게 쫓기게 된다. 그들은 '빨간 망토'라는 데이터 카우리아에 대한 소문을 듣고 데이터를 탈취하려 했다. 미코는 교묘하게 요원들을 흩뿌리고 술집에 도착한다.\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: 울프 코퍼레이션의 CEO의 이름은?\\nAnswer: \"}], 'model': 'gpt-3.5-turbo', 'n': 1, 'stream': False, 'temperature': 0.7}}\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7fb7e8999af0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x7fb84137a6c0> server_hostname='api.openai.com' timeout=None\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7fb843490700>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 26 Nov 2023 13:02:26 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-0613'), (b'openai-organization', b'user-lj8qeahdj7njstnyft5lsza9'), (b'openai-processing-ms', b'2735'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'200'), (b'x-ratelimit-limit-tokens', b'40000'), (b'x-ratelimit-limit-tokens_usage_based', b'40000'), (b'x-ratelimit-remaining-requests', b'196'), (b'x-ratelimit-remaining-tokens', b'39641'), (b'x-ratelimit-remaining-tokens_usage_based', b'39641'), (b'x-ratelimit-reset-requests', b'22m53.338s'), (b'x-ratelimit-reset-tokens', b'538ms'), (b'x-ratelimit-reset-tokens_usage_based', b'538ms'), (b'x-request-id', b'd799a3662ed800229250268c0465c761'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'82c25354fd52ee01-ICN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Request: POST https://api.openai.com/v1/chat/completions \"200 OK\"\n",
      "DEBUG:llama_index.llm_predictor.base:울프 코퍼레이션의 CEO의 이름은 교활한 울프 박사입니다.\n",
      "울프 코퍼레이션의 CEO의 이름은 교활한 울프 박사입니다.\n"
     ]
    }
   ],
   "source": [
    "print(query_engine.query(\"울프 코퍼레이션의 CEO의 이름은?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama Hub\n",
    "\n",
    "라마허브를 사용하면 다양한 데이터를 라마인덱스에서 독자적으로 사용할 수 있음. 위키피디아, 구글 드라이브, 구글 독스, gmail, 노션, spotify, youtube 등의 플러그인들처럼 텍스트 외에도 다양한 파일, 웹서비스 등을 문서의 데이터 소스로 활용할 수 있음.\n",
    "\n",
    "![llama_hub.png](./image/llama_hub.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Youtube Transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.youtube.com:443\n",
      "DEBUG:urllib3.connectionpool:https://www.youtube.com:443 \"GET /watch?v=zPsiPvNod08 HTTP/1.1\" 200 None\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.youtube.com:443\n",
      "DEBUG:urllib3.connectionpool:https://www.youtube.com:443 \"GET /api/timedtext?v=zPsiPvNod08&ei=STZjZZ_5G_GXvcAPvfuLwA4&caps=asr&opi=112496729&xoaf=5&hl=ko&ip=0.0.0.0&ipbits=0&expire=1701025977&sparams=ip,ipbits,expire,v,ei,caps,opi,xoaf&signature=677F958674E411B79B85014473C662BA0DFAB10D.91A3A0A5689CAA2A6DB05E733740A9E7FD4BA43F&key=yt8&lang=en HTTP/1.1\" 200 None\n"
     ]
    }
   ],
   "source": [
    "from llama_index import download_loader\n",
    "\n",
    "YoutubeTranscriptReader = download_loader(\"YoutubeTranscriptReader\")\n",
    "loader = YoutubeTranscriptReader()\n",
    "yt_transcript = loader.load_data(ytlinks=[\"https://youtu.be/zPsiPvNod08?si=3pxE0QzthzMI_dEd\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='/Users/sean_forhim/opt/anaconda3/envs/llama_index/lib/python3.9/site-packages/certifi/cacert.pem'\n",
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='/Users/sean_forhim/opt/anaconda3/envs/llama_index/lib/python3.9/site-packages/certifi/cacert.pem'\n",
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='/Users/sean_forhim/opt/anaconda3/envs/llama_index/lib/python3.9/site-packages/certifi/cacert.pem'\n",
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='/Users/sean_forhim/opt/anaconda3/envs/llama_index/lib/python3.9/site-packages/certifi/cacert.pem'\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Oh, great, great\n",
      "I came to this place a lot\n",
      "wit...\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: Yes, yes, that, yes\n",
      "This guy is..\n",
      "from a star.....\n",
      "DEBUG:llama_index.node_parser.node_utils:> Adding chunk: All you need is a leg\n",
      "of a small octopus\n",
      "Leg of...\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x7f984018d280>, 'json_data': {'input': [\"video_id: zPsiPvNod08  Oh, great, great I came to this place a lot with other teachers The octopus here.. Don't even mention it It's not a joke I can see from this place What do you say.. this place has a Michelin vibe Michelin vibe The vibe! One Love Alpine Club V.. Vibe? There's a vibe Means power, power Power! God damn it! Damn - Oh, why? - Hello I said I wanted Baeksook *Korean chicken soup why are you keep coming to a place like this? No, that's not it... Small octopus here is... I said I wanted Baeksook! We just wanted something healthy for.. I said I wanted to Baeksook! Baeksook is when we go to mount.. I said I wanted to Baeksook! You need to eat something like small oc.. I said I wanted to Baeksook! There are other things that are goo.. I said I wanted Baeksook! You have only a few days left until your checkups, man God damn it I said I wanted Baeksook, but no one's listening to the leader All done, all done Just lower, lower, lower your voice Just don't be that Just.. cool down a bit.. okay? If you want something healthy, You need to eat Baeksook, I said Small octopus is also very good for your stamina You just try it Yeah, the sucker strength of small octopus is crazy so let's just try this healthy food together This is a total pain in the a** The small octopus place today, you know? we're all here today, and a teach Jeong Gwangyong is going to get us some nice healthy food Yes, yes Let's have a great time Who's going to order? It's me, right here Hehehehehehehehe What a stud you are Put a smile on your face, Leader Youngnam Yes, put a smile on your face I've always been telling you this but when the weather is getting hot like this this is when you sweat and lose your strength And we got only few days left until our health checkups This is when you have to treat yourself with good food so that our energies get fully charged Yes, yes, that's so true Get it? But when you're talking about a healthy food of course a small octopus is good but Baeksook is the one for it Get it? I said, we eat a lot of Baeksook when we go to mountain so let's eat what Mr. Jeong has recommended us today, okay? Small octopus You need to take care of your health real nice, you know That's right Tomato juice is very good for antioxidants Do.. Do you have tomato juice at.. Try these side dishes, these are great - Okay, okay, okay - Try it, try it, try it Try it, try it stop talking You... Try this, This Hey, try this, here There you go~ I mean, in the states Their eating habits are all about protein, you know Ohhhhh They have good strength but their big So you need to eat a lot of vegetables So, protein, carb, and fat you say P,C,F Carb... P,C,F Yes I only have fat here This dude.. You're talking nonsense again Eat up, eat up You.. are.. Try it, try it Oh, okay, okay, okay Eat up, eat up But if you look at health, the reason why I'm the most healthy is beCAUSE  I have a good habit of eathing health products I saw him eating like tens and dozens of pills Yes, I eat dozens of pills a day I know, yes But I'm going to recommend you the most crucial ones This is called Nutine Yes This is from a cousin of a friend in my hometown and this is a global brand Oh, that's a great relationship you have Yes, yes, yes Isn't Kim Soohyun a model of this brand? Yes, yes, that, yes This guy is.. from a star... *K-drama 'You came from the stars' Yes, that is correct Yes, yes, yes He's a model Oh, he's the model?\", \"video_id: zPsiPvNod08  Yes, yes, that, yes This guy is.. from a star... *K-drama 'You came from the stars' Yes, that is correct Yes, yes, yes He's a model Oh, he's the model? The nutrients you need not too much, not too little It's designed like that giving us the right nutritional balance That's what this is Wow, I love their slogan What's that? 'New habits to build health' 'My New Routine, Nutine' That's great, right? It means a New Routine Yes, yes, that's cool the meaning is awesome Now, what you do is before you eat, This is a vegetable rTG omega3 80 Vege... What? What? Vegetable Omega3 Vegetable Omega3! Try two pills each Yes? Try two pills each You eat this first? Eat this first, eat this first This, give me it I've seen this before This is good for your blood circulation and eyes It's a big help for them Blood? Blood? Blood circulation Blood circulation! Blood circulation Taekjo, you said your eyes weren't so great Yeah, my eyes were kind of blurry I couldn't really see well at night because I'm watching Youtube all the time Try this, eat this I just need to eat this? Yeah, try these two two pills at once Gwangyong is just chewing on them Wow This is the size is small it's going in real smooth into my throat - Really? - Right? The ones from the state are huge It's cause their throats are this wide It's really hard to swallow Yes, okay, let's eat now, let's eat Wow, look at this small octopus Isn't this a regular octopus? How fresh Look at its strength, look at it It looks like it's going to taste great You just need to feed one of this when a cow is knocked down back in the days You just slide one in their mouth and they just stood up That's right, that's right That stood right up That's why they were like 'Mooo' when they stood up like 'Mooo' 'Mooo' my a** All you need is a chicken I said I didn't want this This is really nice... I'd rather have a chicken leg instead of a leg of octopus Stop it, that's enough You can take Nutine for your nutrients Yeah, you can just eat them if you want to be healthy You can eat both Nutine and Baeksook Then you go eat that! I said I wanted Baeksook! Stop it I said I wanted Baeksook! Stop it I said I wanted Baeksook! Stop it Oh, for f**k sake f**king Baeksook... for the hundredth time We're good, we're good now Just order me one chicken for me Flipping heck, what a pain in the a** Who are you saying is a pain in the a**, damn What do you mean by pain in the a** wHaT dO yOu MeAn bY pAiN In tHe a** That's enough You gotta age nicely I will Woah.. Wow, this is great Oh, this is delightful Wow Wow.. Argh Damn Wow... This is delightful Wow What do you mean by a chicken leg? All you need is a leg of a small octopus Leg of octopus, leg of octopus It's a small octopus, small octopus Oh, yeah small octopus Leg of small octopus, small octopus This is the best Hoo.... Ahhhh Mmm~ Baeksook is so much better Why are you sweating so much? You try this... Mmm, this is good You eat them all Mmm, you try it, too You eat them all Slow down, slow down you Darn you Pour me some That's enough, that's enough Hey, that's enough I'm gonna get drunk Argh Argh, that was nice, right? It was tasty Whew, that was really nice Ah, it was great Yes, the small octopus is so delicious, right?\", \"video_id: zPsiPvNod08  All you need is a leg of a small octopus Leg of octopus, leg of octopus It's a small octopus, small octopus Oh, yeah small octopus Leg of small octopus, small octopus This is the best Hoo.... Ahhhh Mmm~ Baeksook is so much better Why are you sweating so much? You try this... Mmm, this is good You eat them all Mmm, you try it, too You eat them all Slow down, slow down you Darn you Pour me some That's enough, that's enough Hey, that's enough I'm gonna get drunk Argh Argh, that was nice, right? It was tasty Whew, that was really nice Ah, it was great Yes, the small octopus is so delicious, right? I told you it was good Let's go, let's go There are still some octopus left there Thank you for the meal Thank you so much Thank you so very much, right? Yes, that's right That was great Ahh, how nice My tummy is full What fancy thing are you eating by yourself? Give me some Damn it, you're.. Try it What is this It's called Tart Cherry Collagen This is bloody good for you Eat stuff like this instead of coffee you take care of your body with things like this Ahh, but this is it'd be perfect if you eat this as a snack - Right? - Yes This is 100% american tart cherry But this is a bit sour for me This cannot not be sour because The fruit content is very very high so that's why the taste is sour like this You need to deal with it Ahh, it's so nice to have this after small octopus What do you mean it's good to have this with small octopus? It will be so much better with Beaksook Hey, we can't eat this with Baeksook everyday It's so much better with Baeksook! It's so much better with Baeksook! Cut it out, cut it out now - It's so much better with Baeksook! - Stop it, stop it, Youngnam You stop it, you had enough today What do you mean you had enough? You need to know the line for god sake! We just had small octopu... Octopus... If you keep yelling in a public place, people are looking at us Did you just say people are looking, you bastard? Who are you calling a bastard, man? We're all over 60 years old If you keep on saying that just because you're the leader What did you just say right now? I said stop it, you! Who are you yelling at? St.. Stop it... That's enough, enough What's wrong with that dude? You need to know the limit! You're the one with weird personality - Enough, enough, enough - Cut it out Enough, enough, enough You stop living like that right now Do you know how much I'm praying for you? Enough, enough One Love Alpine Club Enough, enough I mean, he's the one who did wrong... - Hey, hey! - This isn't the first ti... I get it, I get it This isn't the first time, you know I totally agree with you But still, think about it, He should be leading us.. Ayyy, you're the best Mr. Bae is the best, the best If he does this one more time, I'm.. Hey! I won't forget this Mr. Bae There, there I'm fine now! Okay, okay There, there Hehehehe This video contains paid advertisements of Nutine\"], 'model': <OpenAIEmbeddingModeModel.TEXT_EMBED_ADA_002: 'text-embedding-ada-002'>, 'encoding_format': 'base64'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=60.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f98401954c0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x7f98810e8dc0> server_hostname='api.openai.com' timeout=60.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f9840195490>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 26 Nov 2023 12:12:59 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'user-lj8qeahdj7njstnyft5lsza9'), (b'openai-processing-ms', b'46'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'200'), (b'x-ratelimit-limit-tokens', b'150000'), (b'x-ratelimit-remaining-requests', b'199'), (b'x-ratelimit-remaining-tokens', b'147590'), (b'x-ratelimit-reset-requests', b'7m12s'), (b'x-ratelimit-reset-tokens', b'964ms'), (b'x-request-id', b'129761892d450fe1dfa4375bc2b1a9ed'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=3mlUxW7o5jFn0vMqZrOjW_0w0y3vRqK_Ub1Rfs7jyPs-1701000779-0-Acv03567Fw+bWj2sgsh2KDF48q16KEB1W6MIvxPWNUUrodWRJl+I9adkZm1wnYdpBN5TGa98SST41F7Kg3cawGA=; path=/; expires=Sun, 26-Nov-23 12:42:59 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=Raoh86Ai0nf1uigzVOgkNuDYimA5m3Q4pXRklHw51Fk-1701000779169-0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'82c20af3ec71310f-ICN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Request: POST https://api.openai.com/v1/embeddings \"200 OK\"\n"
     ]
    }
   ],
   "source": [
    "index1 = GPTVectorStoreIndex.from_documents(yt_transcript)\n",
    "query_engine1 = index1.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x7f9822752280>, 'json_data': {'input': ['이 동영상에서 민수가 가장 먹고 싶어 하는 음식이 무엇인가요?'], 'model': <OpenAIEmbeddingModeModel.TEXT_EMBED_ADA_002: 'text-embedding-ada-002'>, 'encoding_format': 'base64'}}\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=60.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f98401c3220>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x7f98810e8dc0> server_hostname='api.openai.com' timeout=60.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f98401c3520>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 26 Nov 2023 12:13:05 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'user-lj8qeahdj7njstnyft5lsza9'), (b'openai-processing-ms', b'24'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'200'), (b'x-ratelimit-limit-tokens', b'150000'), (b'x-ratelimit-remaining-requests', b'198'), (b'x-ratelimit-remaining-tokens', b'149979'), (b'x-ratelimit-reset-requests', b'14m18.096s'), (b'x-ratelimit-reset-tokens', b'8ms'), (b'x-request-id', b'624924244b7fc16af1966bd7b44871a7'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'82c20b18f8c93278-ICN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Request: POST https://api.openai.com/v1/embeddings \"200 OK\"\n",
      "DEBUG:llama_index.indices.utils:> Top 2 nodes:\n",
      "> [Node e6cdbcf9-0be9-4e42-a653-36a6855a0719] [Similarity score:             0.788616] Yes, yes, that, yes\n",
      "This guy is..\n",
      "from a star...\n",
      "*K-drama 'You came from the stars'\n",
      "Yes, that is ...\n",
      "> [Node b0593245-070e-43fb-b7f5-b6207044af3b] [Similarity score:             0.769485] Oh, great, great\n",
      "I came to this place a lot\n",
      "with other teachers\n",
      "The octopus here..\n",
      "Don't even men...\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': <MessageRole.SYSTEM: 'system'>, 'content': \"You are an expert Q&A system that is trusted around the world.\\nAlways answer the query using the provided context information, and not prior knowledge.\\nSome rules to follow:\\n1. Never directly reference the given context in your answer.\\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\"}, {'role': <MessageRole.USER: 'user'>, 'content': \"Context information is below.\\n---------------------\\nvideo_id: zPsiPvNod08\\n\\nYes, yes, that, yes\\nThis guy is..\\nfrom a star...\\n*K-drama 'You came from the stars'\\nYes, that is correct\\nYes, yes, yes\\nHe's a model\\nOh, he's the model?\\nThe nutrients you need\\nnot too much,\\nnot too little\\nIt's designed like that\\ngiving us\\nthe right nutritional balance\\nThat's what this is\\nWow, I love their slogan\\nWhat's that?\\n'New habits to build health'\\n'My New Routine, Nutine'\\nThat's great, right?\\nIt means a New Routine\\nYes, yes, that's cool\\nthe meaning is awesome\\nNow, what you do is\\nbefore you eat,\\nThis is a vegetable\\nrTG omega3 80\\nVege...\\nWhat? What?\\nVegetable Omega3\\nVegetable Omega3!\\nTry two pills each\\nYes? Try two pills each\\nYou eat this first?\\nEat this first,\\neat this first\\nThis, give me it\\nI've seen this before\\nThis is good for your\\nblood circulation and eyes\\nIt's a big help for them\\nBlood? Blood?\\nBlood circulation\\nBlood circulation!\\nBlood circulation\\nTaekjo, you said\\nyour eyes weren't so great\\nYeah, my eyes were\\nkind of blurry\\nI couldn't really\\nsee well at night\\nbecause I'm watching\\nYoutube all the time\\nTry this, eat this\\nI just need to eat this?\\nYeah, try these two\\ntwo pills at once\\nGwangyong is just\\nchewing on them\\nWow\\nThis is\\nthe size is small\\nit's going in real smooth\\ninto my throat\\n- Really?\\n- Right?\\nThe ones from\\nthe state are huge\\nIt's cause their\\nthroats are this wide\\nIt's really hard to swallow\\nYes, okay,\\nlet's eat now, let's eat\\nWow, look at\\nthis small octopus\\nIsn't this a regular octopus?\\nHow fresh\\nLook at its strength,\\nlook at it\\nIt looks like\\nit's going to taste great\\nYou just need to feed\\none of this when\\na cow is knocked down\\nback in the days\\nYou just slide one\\nin their mouth\\nand they just stood up\\nThat's right,\\nthat's right\\nThat stood right up\\nThat's why\\nthey were like 'Mooo'\\nwhen they stood up\\nlike 'Mooo'\\n'Mooo' my a**\\nAll you need\\nis a chicken\\nI said I didn't want this\\nThis is really nice...\\nI'd rather have a chicken leg\\ninstead of a leg of octopus\\nStop it, that's enough\\nYou can take Nutine\\nfor your nutrients\\nYeah, you can just\\neat them if you want to be healthy\\nYou can eat both\\nNutine and Baeksook\\nThen you go eat that!\\nI said I wanted Baeksook!\\nStop it\\nI said I wanted Baeksook!\\nStop it\\nI said I wanted Baeksook!\\nStop it\\nOh, for f**k sake\\nf**king Baeksook...\\nfor the hundredth time\\nWe're good,\\nwe're good now\\nJust order me\\none chicken for me\\nFlipping heck,\\nwhat a pain in the a**\\nWho are you saying\\nis a pain in the a**, damn\\nWhat do you mean\\nby pain in the a**\\nwHaT dO yOu MeAn\\nbY pAiN In tHe a**\\nThat's enough\\nYou gotta age nicely\\nI will\\nWoah..\\nWow, this is great\\nOh, this is delightful\\nWow\\nWow..\\nArgh\\nDamn\\nWow...\\nThis is delightful\\nWow\\nWhat do you mean\\nby a chicken leg?\\nAll you need is a leg\\nof a small octopus\\nLeg of octopus,\\nleg of octopus\\nIt's a small octopus,\\nsmall octopus\\nOh, yeah\\nsmall octopus\\nLeg of small octopus,\\nsmall octopus\\nThis is the best\\nHoo....\\nAhhhh\\nMmm~\\nBaeksook is so much better\\nWhy are you sweating so much?\\nYou try this...\\nMmm, this is good\\nYou eat them all\\nMmm,\\nyou try it, too\\nYou eat them all\\nSlow down,\\nslow down you\\nDarn you\\nPour me some\\nThat's enough,\\nthat's enough\\nHey, that's enough\\nI'm gonna get drunk\\nArgh\\nArgh, that was nice, right?\\nIt was tasty\\nWhew, that was\\nreally nice\\nAh, it was great\\nYes, the small octopus is\\nso delicious, right?\\n\\nvideo_id: zPsiPvNod08\\n\\nOh, great, great\\nI came to this place a lot\\nwith other teachers\\nThe octopus here..\\nDon't even mention it\\nIt's not a joke\\nI can see from this place\\nWhat do you say..\\nthis place\\nhas a Michelin vibe\\nMichelin vibe\\nThe vibe!\\nOne Love Alpine Club\\nV.. Vibe?\\nThere's a vibe\\nMeans power, power\\nPower!\\nGod damn it!\\nDamn\\n- Oh, why?\\n- Hello\\nI said I wanted Baeksook\\n*Korean chicken soup\\nwhy are you keep\\ncoming to a place like this?\\nNo, that's not it...\\nSmall octopus here is...\\nI said I wanted Baeksook!\\nWe just wanted\\nsomething healthy for..\\nI said I wanted to Baeksook!\\nBaeksook is\\nwhen we go to mount..\\nI said I wanted to Baeksook!\\nYou need to eat\\nsomething like small oc..\\nI said I wanted to Baeksook!\\nThere are other things\\nthat are goo..\\nI said I wanted Baeksook!\\nYou have only a few days\\nleft until your checkups, man\\nGod damn it\\nI said I wanted Baeksook,\\nbut no one's listening to the leader\\nAll done,\\nall done\\nJust lower,\\nlower, lower your voice\\nJust don't be that\\nJust.. cool down a bit..\\nokay?\\nIf you want\\nsomething healthy,\\nYou need to\\neat Baeksook, I said\\nSmall octopus is also\\nvery good for your stamina\\nYou just try it\\nYeah, the sucker strength\\nof small octopus is crazy\\nso let's just try this\\nhealthy food together\\nThis is a total pain in the a**\\nThe small octopus\\nplace today, you know?\\nwe're all here today,\\nand a teach Jeong Gwangyong\\nis going to get us\\nsome nice healthy food\\nYes, yes\\nLet's have a great time\\nWho's going to order?\\nIt's me, right here\\nHehehehehehehehe\\nWhat a stud you are\\nPut a smile on your face,\\nLeader Youngnam\\nYes, put a smile\\non your face\\nI've always been\\ntelling you this but\\nwhen the weather\\nis getting hot like this\\nthis is when you sweat\\nand lose your strength\\nAnd we got only few days left\\nuntil our health checkups\\nThis is when you have to\\ntreat yourself with good food\\nso that our energies\\nget fully charged\\nYes, yes,\\nthat's so true\\nGet it?\\nBut when you're talking\\nabout a healthy food\\nof course\\na small octopus is good\\nbut Baeksook is the one for it\\nGet it?\\nI said, we eat a lot of Baeksook\\nwhen we go to mountain\\nso let's eat what Mr. Jeong\\nhas recommended us today, okay?\\nSmall octopus\\nYou need to take care of\\nyour health real nice, you know\\nThat's right\\nTomato juice is\\nvery good for antioxidants\\nDo.. Do you have\\ntomato juice at..\\nTry these side dishes,\\nthese are great\\n- Okay, okay, okay\\n- Try it, try it, try it\\nTry it, try it\\nstop talking\\nYou...\\nTry this, This\\nHey, try this, here\\nThere you go~\\nI mean, in the states\\nTheir eating habits are\\nall about protein, you know\\nOhhhhh\\nThey have good strength\\nbut their big\\nSo you need to eat\\na lot of vegetables\\nSo, protein, carb, and fat\\nyou say P,C,F\\nCarb...\\nP,C,F\\nYes\\nI only have fat here\\nThis dude..\\nYou're talking nonsense again\\nEat up, eat up\\nYou.. are..\\nTry it, try it\\nOh, okay, okay, okay\\nEat up, eat up\\nBut if you look at health,\\nthe reason why I'm\\nthe most healthy\\nis beCAUSE\\n I have a good habit\\nof eathing health products\\nI saw him eating like\\ntens and dozens of pills\\nYes, I eat dozens of pills a day\\nI know, yes\\nBut I'm going to recommend\\nyou the most crucial ones\\nThis is called Nutine\\nYes\\nThis is from\\na cousin of a friend\\nin my hometown\\nand this is a global brand\\nOh, that's a great\\nrelationship you have\\nYes, yes, yes\\nIsn't Kim Soohyun\\na model of this brand?\\nYes, yes, that, yes\\nThis guy is..\\nfrom a star...\\n*K-drama 'You came from the stars'\\nYes, that is correct\\nYes, yes, yes\\nHe's a model\\nOh, he's the model?\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: 이 동영상에서 민수가 가장 먹고 싶어 하는 음식이 무엇인가요?\\nAnswer: \"}], 'model': 'gpt-3.5-turbo', 'stream': False, 'temperature': 0.1}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=60.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f98401cb970>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x7f98810e8d40> server_hostname='api.openai.com' timeout=60.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f98401cb940>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 26 Nov 2023 12:13:08 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-0613'), (b'openai-organization', b'user-lj8qeahdj7njstnyft5lsza9'), (b'openai-processing-ms', b'2540'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'200'), (b'x-ratelimit-limit-tokens', b'40000'), (b'x-ratelimit-limit-tokens_usage_based', b'40000'), (b'x-ratelimit-remaining-requests', b'193'), (b'x-ratelimit-remaining-tokens', b'38157'), (b'x-ratelimit-remaining-tokens_usage_based', b'38157'), (b'x-ratelimit-reset-requests', b'43m23.677s'), (b'x-ratelimit-reset-tokens', b'2.764s'), (b'x-ratelimit-reset-tokens_usage_based', b'2.764s'), (b'x-request-id', b'c9ece79d5026228199e4c480b46c1188'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=bL1r5GlDfd2NFLf7T8gHPPY2KHPI9U51WkJZkpYYG.U-1701000788-0-AdpmHEjbN+3uP+FA1c+19NSlRyt7CXE4MT0Hil+ZvTwMFgx2U74s/f0W9Labv6qFFs+83d0CnNWHNaMzNpPiyTA=; path=/; expires=Sun, 26-Nov-23 12:43:08 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=48b5.pU6qqnoB7VoJGdvk5CKPnx1g9.fZmZ7858xn1E-1701000788157-0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'82c20b1b3b7b00d3-ICN'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Request: POST https://api.openai.com/v1/chat/completions \"200 OK\"\n",
      "DEBUG:llama_index.llm_predictor.base:민수가 가장 먹고 싶어 하는 음식은 백숙입니다.\n",
      "민수가 가장 먹고 싶어 하는 음식은 백숙입니다.\n"
     ]
    }
   ],
   "source": [
    "print(query_engine1.query(\"이 동영상에서 민수가 가장 먹고 싶어 하는 음식이 무엇인가요?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama Index VS Langchain\n",
    "\n",
    "라마인덱스는 랭체인 라이브러리로 구축된 애플리케이션.\n",
    "랭체인은 조금 더 전반적인 기능들을 다룰 수 있음. loading, processing, indexing, LLM과의 interaction 등. 이로써 사용자는 사용자의 의도에 따라 커스터마이징에 유리하다.\n",
    "반면, 라마인덱스는 search and retrieval에 초점을 맞춘 애플리케이션으로 거대한 양의 데이터를 처리하기에 좋음."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
